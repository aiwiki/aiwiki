---
layout: post
title: "ML 2: Linear Regression v√† gi·ªõi thi·ªáu v·ªÅ Gradient Descent."
categories: [Machine Learning, Math, AI]
author: Bagumeow
usemathjax: true
tags:
  [bagumeow, machinelearning, supervised, gradientdescent, sklearn]
description: "T·ª´ Linear (tuy·∫øn t√≠nh) xu·∫•t ph√°t t·ª´ linearis trong ti·∫øng Latinh, c√≥ nghƒ©a l√† th·ª© g√¨ ƒë√≥ li√™n quan ho·∫∑c t∆∞∆°ng t·ª± ƒë∆∞·ªùng th·∫≥ng. - ƒÉn c·∫Øp t·ª´ Thanh H√≤a"
---
> L√≠ do m√† m√¨nh vi·∫øt blog v√¨ mu·ªën chia s·∫ª ki·∫øn th·ª©c m√† m√¨nh hi·ªÉu ƒë√©n v·ªõi m·ªçi ng∆∞·ªùi üòÅüòÅ

## I. Gi·ªõi thi·ªáu

V·∫≠y **Linear regression** l√† g√¨? B·∫£n ch·∫•t t·ª´ **Linear** (tuy·∫øn t√≠nh) xu·∫•t ph√°t t·ª´ *linearis* trong ti·∫øng Latinh, c√≥ nghƒ©a l√† *th·ª© g√¨ ƒë√≥ li√™n quan ho·∫∑c t∆∞∆°ng t·ª± ƒë∆∞·ªùng th·∫≥ng.* ƒê√¢y l√† m·ªôt k·ªπ thu·∫≠t ph√¢n t√≠ch d·ªØ li·ªáu s·ª≠ d·ª•ng m·ªôt hay nhi·ªÅu bi·∫øn ƒë·ªôc l·∫≠p X (data) ƒë·ªÉ d·ª± ƒëo√°n bi·∫øn ph·ª• thu·ªôc Y (label). N√≥i ƒë∆°n gi·∫£n d·ª±a v√†o X ta s·∫Ω ƒëi t√¨m m·ªôt ƒë∆∞·ªùng th·∫≥ng (ho·∫∑c [si√™u ph·∫≥ng](https://en.wikipedia.org/wiki/Hyperplane)) c√≥ th·ªÉ ƒëi qua to√†n b·ªô ƒëi·ªÉm d·ªØ li·ªáu v·ªõi ƒëi·ªÅu ki·ªán r·∫±ng b·ªô d·ª± li·ªáu c√≥ ƒë·ªô t∆∞∆°ng quan l·ªõn v·ªõi nhau.

S·ªë chi·ªÅu c·ªßa si√™u ph·∫≥ng m√¥ h√¨nh m·ªëi quan h·ªá gi·ªØa c√°c bi·∫øn ph·ª• thu·ªôc v√†o s·ªë chi·ªÅu c·ªßa d·ªØ li·ªáu (t√≠nh c·∫£ nh√£n). C√≥ nghƒ©a l√† n·∫øu d·ªØ li·ªáu b·∫°n c√≥ l√† 2 chi·ªÅu th√¨ si√™u ph·∫≥ng bi·ªÉu di·ªÖn m·ªëi quan h·ªá s·∫Ω l√† m·ªôt ƒë∆∞·ªùng th·∫≥ng, Hai chi·ªÅu th√¨ s·∫Ω l√† m·ªôt m·∫∑t ph·∫≥ng, B·ªën chi·ªÅu s·∫Ω l√† m·ªôt kh√¥ng gian ba chi·ªÅu‚Ä¶

Nh∆∞ v·∫≠y n·∫øu d·ªØ li·ªáu ta c√≥ $dim=n$ th√¨ si√™u ph·∫≥ng bi·ªÉu di·ªÖn m·ªëi quan h·ªá s·∫Ω c√≥ $dim = n-1$

<div style="text-align: center;">
    <img src="../assets/ml_2/ex_linear1.png" alt="·∫¢nh ƒë∆∞·ª£c sinh b·ªüi Bing Image Creator üòÅ" width="300" style="display: inline-block;">
    <img src="../assets/ml_2/ex_linear2.png" alt="·∫¢nh ƒë∆∞·ª£c sinh b·ªüi Bing Image Creator üòÅ" width="300" style="display: inline-block;">
</div>
<figcaption style="text-align: center;"><i>H√¨nh 1. V√≠ d·ª• v·ªÅ Linear regression trong kh√¥ng gian 1 v√† 2 chi·ªÅu</i></figcaption>


L·∫•y m·ªôt d·ªØ li·ªáu hai chi·ªÅu l√†m v√≠ d·ª• nh∆∞ sau: B·∫°n mu·ªën ƒë·∫∑t **XanhSM** (m√¨nh s·∫Ω gi·∫£ s·ª≠ XanhSM c√≥ ph∆∞∆°ng ti·ªán v·ªõi c√¥ng ngh·ªá t∆∞∆°ng lai, c√≥ th·ªÉ ƒëi v·ªõi t·ªëc ƒë·ªô √°nh s√°ng ü§°) ƒë·ªÉ ƒëi t·ª´ IUH ƒë·∫øn Gigamall v·ªõi qu·∫£ng ƒë∆∞·ªùng $x =4km$ trong **8 ph√∫t** .Nh∆∞ng kh√¥ng may h√¥m nay h·ªá th·ªëng t√≠nh ti·ªÅn b·ªã l·ªói v√† b·∫°n kh√¥ng bi·∫øt gi√° ti·ªÅn nh∆∞ th·∫ø n√†o.

May thay b·∫°n v·∫´n c√≥ th·ªÉ truy c·∫≠p v√†o l·ªãch s·ª≠ c√°c chuy·∫øn ƒëi c·ªßa b·∫°n v√† c√≥ d·ªØ li·ªáu nh∆∞ sau :


| S·ªë Km(Km)  | Th·ªùi gian (Ph√∫t) | S·ªë ti·ªÅn ph·∫£i tr·∫£ (1000 VND) |
| --- | --- | --- |
| 12 | 24 | 60 |
| 2 | 4 | 13 |
| 6 | 25 | 20 |
| 7 | 13 | 35 |
| 9 | 17 | 40 |
| 15 | 10 | 125 |
| 1 | 3 | 10 |
| 20 | 40 | 90 |
| 18 | 4 | 300 |
| 50 | 90 | 202 |
| 31 | 62 | 128 |
| 14 | 28 | 72 |

Gi·∫£ s·ª≠ **s·ªë km** b·∫°n ƒëi v√† **s·ªë ti·ªán b·∫°n tr·∫£** ph·ª• thu·ªôc tuy·∫øn t√≠nh v·ªõi nhau th√¨ b·∫°n c√≥ t√¨m ra ƒë∆∞·ª£c m·ªôt h√†m bi·ªÉu th·ªã m·ªëi quan h·ªá gi·ªØa hai ƒë·∫°i l∆∞·ª£ng tr√™n kh√¥ng? N·∫øu c√≥ th√¨ h√†m d·ª± ƒëo√°n s·∫Ω nh∆∞ th·∫ø n√†o? ·ªû ƒë√¢y d·ªØ li·ªáu c·ªßa m√¨nh s·∫Ω ƒë·∫∑t l√† $\color{teal}{\mathbf{x}} \color{white}{= [x_1,x_2] }$, v·ªõi $x_1$ bi·ªÉu di·ªÖn s·ªë km, $x_2$  bi·ªÉu di·ªÖn s·ªë ph√∫t. 
B·∫°n c√≥ th·ªÉ ƒë·ªÉ √Ω r·∫±ng  n·∫øu s·ªë km ho·∫∑c th·ªùi gian c√†ng ng·∫Øn th√¨ s·ªë ti·ªÅn ph·∫£i tr·∫£ s·∫Ω c√†ng nhi·ªÅu.

·ªû b√†i tr∆∞·ªõc m√¨nh ƒë√£ n√≥i v·ªÅ thu·∫≠t to√°n Supervised Learning th√¨ h√†m m√¨nh c·∫ßn bi·ªÉu di·ªÖn s·∫Ω c√≥ m·ª•c ƒë√≠ch nh∆∞ sau: 

$$
\color{Lime}{y}\ \color{None}{\approx} \ \color{orange}{f(}\color{teal}{\mathbf{x}}\color{orange}{)} \ \color{None}{=} \ \color{hotpink}{\hat{y}}
$$

$$
\color{orange}{f(}\color{teal}{\mathbf{x}}\color{orange}{)} \color{Azure} = w_1 x_1 + w_2 x_2 +  w_0 ~~~~ (1)
$$

trong ƒë√≥ $\{w_1,w_2,w_0\}$ l√† b·ªô tham s·ªë c·∫ßn t·ªëi ∆∞u. $w_0$  (c√≥ th·ªÉ k√≠ hi·ªáu b·∫±ng $\beta$) c√≤n ƒë∆∞·ª£c g·ªçi l√† bias. B√†i to√°n Linear Regresion ƒë·ªãnh nghƒ©a r·∫±ng ta ph·∫£i t√¨m b·ªô tham s·ªë $\{w_1,w_2,w_0\}$ sao cho si√™u ph·∫≥ng $\color{orange}{f(}\color{teal}{\mathbf{x}}\color{orange}{)}$  x·∫•p x·ªâ ƒë∆∞·ª£c $\color{Lime}{y}$ .

## II. Ph√¢n t√≠ch to√°n h·ªçc.

### 2.1. Bi·ªÉu di·ªÖn to√°n h·ªçc.

·ªû c·∫•p ba c√≥ th·ªÉ b·∫°n s·∫Ω bi·∫øt ph∆∞∆°ng tr√¨nh bi·ªÉu di·ªÖn m·ªôt ƒë∆∞·ªùng th·∫≥ng tr√™n m·ªôt h·ªá t·ªça ƒë·ªô $Oxy$  s·∫Ω c√≥ d·∫°ng :  $ax + by + c = 0$ hay $y = ax + b$. Trong ƒë√≥ $a$  l√† ƒë·ªô d·ªëc (slope) $b$ l√† tung ƒë·ªô (ph√≤ng tr∆∞·ªùng h·ª£p b·∫°n qu√™n, m√¨nh s·∫Ω ƒë·ªÉ m·ªôt video si√™u hay t·ª´ V·∫≠t L√Ω Chill [t·∫°i ƒë√¢y](https://youtu.be/mn4JSg1NzOs?si=kJz0E5NrYeb_IcCw))


<figure style="text-align: center;">
  <img src="../assets/ml_2/linearfunction.png" width="600" alt="Description of the image">
  <figcaption><i>H√¨nh 2. ·∫¢nh t·ª´ Video gi·∫£i th√≠ch b·∫£n ch·∫•t c·ªßa Linear Function (<a href="https://youtu.be/mn4JSg1NzOs?si=kJz0E5NrYeb_IcCw">V·∫≠t L√Ω Chill</a>)</i></figcaption>
</figure>


B·∫£n ch·∫•t c·ªßa Linear Regression l√† ta s·∫Ω ƒëi t√¨m b·ªô tham s·ªë sao cho si√™u ph·∫≥ng x·∫•p x·ªâ ƒë∆∞·ª£c gi√° tr·ªã c·ªßa c√°c ƒëi·ªÉm d·ªØ li·ªáu. V√† b·∫°n s·∫Ω th·∫•y ƒë∆∞·ª£c $\{w_1,\cdots, w_n\}$ s·∫Ω bi·ªÉu di·ªÖn cho ƒë·ªô d·ªëc c·ªßa si√™u ph·∫≥ng, v√† $w_0$  hay $\beta$ s·∫Ω bi·ªÉu di·ªÖn cho ƒë·ªô d·ªëc.

Quay l·∫°i v·ªõi v√≠ d·ª• ·ªü tr√™n, b√¢y gi·ªù ta s·∫Ω ƒë·∫∑t $\mathbf{w} = [w_0,w_1,w_2]^T$, $\color{teal}{\bar{\mathbf{x}} = [1,x_1,x_2] }$. L√∫c n√†y <span style="color:hotpink;">gi√° tr·ªã d·ª± ƒëo√°n</span> $\color{hotpink}{\hat{y}}$ s·∫Ω l√† m·ªôt t·ªï h·ª£p tuy·∫øn t√≠nh  t·ª´ $\color{teal}{\bar{\mathbf{x}}}$ v√† $\mathbf{w}$

$$
\color{Lime}{y} \ \color{None}{ \approx}  \ \color{teal}{\mathbf{\bar{x}}} \color{None}{\mathbf{w}} = \color{hotpink}{\hat{y}}
$$

V√† n·∫øu b·∫°n c√≥ th·∫Øc m·∫Øc r·∫±ng ‚ÄúL√†m c√°ch n√†o ƒë·ªÉ bi·∫øt b·ªô tham s·ªë n√†o l√† t·ªët?‚Äù.  T·ª´ ƒë√¢y ta s·∫Ω c√≥ kh√°i ni·ªám **H√†m m·∫•t m√°t.**

### 2.2. H√†m m·∫•t m√°t (loss function)

H·∫ßu nh∆∞ m·ªçi thu·∫≠t to√°n Machine Learning c·∫ßn c√≥ m·ªôt ph∆∞∆°ng th·ª©c ƒë·ªÉ c√≥ th·ªÉ ƒë√°nh gi√° m√¥ h√¨nh ƒë√≥ c√≥ t·ªët hay kh√¥ng, ƒë√≥ s·∫Ω g·ªçi l√† h√†m m·∫•t m√°t,  M·ª•c ƒë√≠ch ch√≠nh c·ªßa h√†m m·∫•t m√°t l√† ƒëo l∆∞·ªùng s·ª± kh√°c bi·ªát gi·ªØa <span style="color:hotpink;">gi√° tr·ªã d·ª± ƒëo√°n</span> c·ªßa m√¥ h√¨nh v√† <span style="color:lime;">gi√° tr·ªã th·ª±c t·∫ø</span> m√† m√¥ h√¨nh c·∫ßn ph·∫£i d·ª± ƒëo√°n. T·ª´ ƒë√≥, h√†m m·∫•t m√°t gi√∫p t·ªëi ∆∞u h√≥a v√† c·∫£i thi·ªán hi·ªáu su·∫•t c·ªßa m√¥ h√¨nh.

Trong b√†i n√†y m√¨nh s·∫Ω gi·ªõi thi·ªáu cho m·ªçi ng∆∞·ªùi h√†m Mean Squared Error(MSE) v·ªõi c√¥ng th·ª©c nh∆∞ sau:

$$
\mathcal{L}(\mathbf{w}) = \frac{1}{2}\sum_{i=1}^N (\color{Lime}{y_i} \color{None} - \color{hotpink}{ \hat{y}}\color{None})^2 = \frac{1}{2}\sum_{i=1}^N (\color{Lime}{y_i} \color{None} - \color{teal}{\mathbf{\bar{x}}} \color{None}{\mathbf{w}})^2 ~~~~~(2) 
$$

*‚ÄúS·ªë $\frac{1}{2}$ l√† h·∫±ng s·ªë kh√¥ng ·∫£nh h∆∞·ªüng g√¨ ƒë·∫øn k·∫øt qu·∫£, m√† s·∫Ω gi√∫p cho ƒë·∫°o h√†m (ph·∫ßn sau) tr·ªü n√™n ƒë·∫πp h∆°n‚Äù*

Ta s·∫Ω mong mu·ªën r·∫±ng s·ª± sai s·ªë gi·ªØa <span style="color:hotpink;">gi√° tr·ªã d·ª± ƒëo√°n</span> v√† <span style="color:lime;">gi√° tr·ªã th·ª±c t·∫ø</span> s·∫Ω l√† nh·ªè nh·∫•t. ƒê·ªìng nghƒ©a v·ªõi vi·ªác t√¨m h·ªá s·ªë $\mathbf{w}$ sao cho $\mathcal{L}(\mathbf{w})$ ƒë·∫°t gi√° tr·ªã nh·ªè nh·∫•t. V√† b·ªô tham s·ªë th·ªèa m√£n ƒëi·ªÅu ki·ªán tr√™n ƒë∆∞·ª£c g·ªçi l√† nghi·ªám t·ªëi ∆∞u c·ªßa b√†i to√°n, k√Ω hi·ªáu l√† :

$$
\mathbf{w}^* = \arg\min_{\mathbf{w}} \mathcal{L}(\mathbf{w})
$$

ƒê·ªÉ qu√° tr√¨nh gi·∫£i nghi·ªám t·ªëi ∆∞u d·ªÖ d√†ng h∆°n th√¨ m√¨nh s·∫Ω ƒë·∫∑t $\color{Lime}{\mathbf{y} = [y_1; y_2; \dots; y_N]} \ \color{None}{\in \mathbb{R}^N }$ , l√† vector ch·ª©a to√†n b·ªô <span style="color:lime;">gi√° tr·ªã th·ª±c t·∫ø</span> trong b·ªô data, $\color{teal}{\mathbf{\bar{X}} = [\mathbf{\bar{x}}_1; \mathbf{\bar{x}}_2; \dots; \mathbf{\bar{x}}_N ]} \ \color{None}{\in \mathbb{R}^{d \times N}}$
 l√† ma tr·∫≠n c·ªßa to√†n b·ªô d·ªØ li·ªáu v·ªõi $d$  l√† s·ªë chi·ªÅu c·ªßa d·ªØ li·ªáu v√† $N$ l√† s·ªë l∆∞·ª£ng ƒëi·ªÉm d·ªØ li·ªáu. Khi n√†y h√†m m·∫•t m√°t s·∫Ω ƒë∆∞·ª£c vi·∫øt d∆∞·ªõi d·∫°ng ma tr·∫≠n t·ªïng qu√°t ƒë∆°n gi·∫£n h∆°n.

$$
\mathcal{L}(\mathbf{w})= \frac{1}{2} \| \color{Lime}{\mathbf{y}} \color{None}{-} \color{teal}{ \mathbf{\bar{X}}} \color{None}{\mathbf{w} \|_2^2} ~~~(3)
$$

ƒê√¢y l√† c√¥ng th·ª©c b√¨nh ph∆∞∆°ng kho·∫£ng c√°ch Euclid  (Chu·∫©n Euclid, Norm 2). T·ª´ ƒë√¢y ta c√≥ th·ªÉ n√≥
i r·∫±ng MSE s·∫Ω so s√°nh vector <span style="color:lime;">gi√° tr·ªã th·ª±c t·∫ø</span> v√† vector <span style="color:hotpink;">gi√° tr·ªã d·ª± ƒëo√°n</span> v·ªõi nhau. Ngo√†i ra Norm 2  c√≥ th·ªÉ khai tri·ªÉn ra nh∆∞ sau:  $\color{Coral}{\|z\|_2 = \sqrt{z^Tz}}$ .

### 2.3. B·ªï sung v·ªÅ to√°n h·ªçc

‚Äú*·ªû ph·∫ßn n√†y th√¨ m√¨nh s·∫Ω n√≥i v·ªÅ c√°c kh√°i ni·ªám to√°n m√† b·∫°n s·∫Ω g·∫∑p n·∫øu mu·ªën hi·ªÉu s√¢u v·ªÅ qu√° tr√¨nh m√† m·ªôt m√¥ h√¨nh m√°y h·ªçc s·∫Ω t·ªëi ∆∞u. V·∫≠y n·∫øu b·∫°n kh√¥ng ƒëam m√™ v·ªÅ to√°n ho·∫∑c kh√¥ng hi·ªÉu n·ªïi th√¨ h√£y skip ph·∫ßn n√†y nh√© :D*‚Äù

M·ªôt b·∫≠t m√≠ nh·ªè cho c√°c b·∫°n n·∫øu ƒëam m√™ v·ªÅ to√°n th√¨ MSE l√† m·ªôt d·∫°ng to√†n ph∆∞∆°ng ([Quadratics Forms](https://en.wikipedia.org/wiki/Quadratic_form)), ƒë√¢y l√† bi·ªÉu di·ªÖn c·ªßa ph∆∞∆°ng tr√¨nh b·∫≠c hai c·ªßa kh√¥ng gian l·ªõn h∆°n 1 chi·ªÅu (t·ª©c l√† ph∆∞∆°ng tr√¨nh b·∫≠c hai th√¥ng th∆∞·ªùng c≈©ng l√† m·ªôt d·∫°ng to√†n ph∆∞∆°ng). V·ªõi bi·∫øn l√† vector $\mathbf{x} = [x_1,x_2,...,x_n] \in \mathbb{R}^n$ th√¨ ph∆∞∆°ng tr√¨nh t·ªïng qu√°t c·ªßa d·∫°ng to√†n ph∆∞∆°ng nh∆∞ sau :  

$$
f(\mathbf{x})=\mathbf{x}^TA\mathbf{x} + b^T\mathbf{x} + c
$$

‚ÄúNh√¨n quen nh·ªâ, tr√¥ng ƒë√¢u kh√°c g√¨ $f(x) = ax^2 + bx +c$   ƒë√¢u üòÅ‚Äú.

V√† t∆∞∆°ng t·ª± nh∆∞ h√†m b·∫≠c 2 v·ªõi ƒë·∫°o h√†m $f(x)^{'} = 2ax +b$  th√¨ ƒë·∫°o h√†m b·∫≠c m·ªôt t·ªïng qu√°t c·ªßa d·∫°ng to√†n ph∆∞∆°ng s·∫Ω l√† $f(\mathbf{x})^{'} = 2Ax + b$. (C√≥ m·ªôt b√†i vi·∫øt m√† blog n√†y ƒë√£ chia s·∫ª v·ªÅ ƒë·∫°o h·∫°m, c√°c b·∫°n c√≥ th·ªÉ tham kh·∫£o th√™m t·∫°i ƒë√¢y).

·ªû trong to√°n t·ªëi ∆∞u th√¨ s·∫Ω c√≥ kh√°i ni·ªám g·ªçi l√† ‚ÄúB√†i to√°n t·ªëi ∆∞u l·ªìi‚Äù.  D·ª±a v√†o h√†m c·ªßa b·∫°n l√† h√†m l·ªìi hay h√†m l√µm th√¨ ta s·∫Ω ph·∫£i ƒëi t√¨m ƒëi·ªÉm c·ª±c ti·ªÉu hay c·ª±c ƒë·∫°i c·ªßa ch√∫ng.

<div style="text-align: center;">
    <img src="../assets/ml_2/convex.png" alt="convex" width="300" style="display: inline-block;">
    <img src="../assets/ml_2/concave.png" alt="concave" width="300" style="display: inline-block;">
</div>
<figcaption style="text-align: center;"><i>H√¨nh 3. V√≠ d·ª• v·ªÅ h√†m l·ªìi (b√™n tr√°i) v√† h√†m l√µm (b√™n ph·∫£i)</i></figcaption>

V·∫≠y th√¨ h√†m c√≥ d·∫°ng to√†n ph∆∞∆°ng l√† h√†m l·ªìi (convex function) hay h√†m l√µm (concave function)? Ta ch∆∞a tr·∫£ l·ªùi ƒë∆∞·ª£c nh∆∞ng ƒë∆°n gi·∫£n l·∫Øm, b·∫°n c√≥ ƒë·ªÉ √Ω th·∫•y ma tr·∫≠n $A \in \mathbb{R}^{n \times n}$ ·ªü trong h√†m to√†n ph∆∞∆°ng kh√¥ng, c·ª© coi n√≥ l√† h·ªá s·ªë g√≥c ƒë·∫•y.  Ma tr·∫≠n ƒë√≥ th∆∞·ªùng l√† m·ªôt ma tr·∫≠n ƒë·ªëi x·ª©ng. Nh∆∞ng ƒë·ªëi v·ªõi kh√¥ng gian l·ªõn h∆°n 1 chi·ªÅu th√¨ s·∫Ω c√≥ kh√°i ni·ªám ***n·ª≠a x√°c ƒë·ªãnh d∆∞∆°ng, x√°c ƒë·ªãnh d∆∞∆°ng, n·ª≠a x√°c ƒë·ªãnh √¢m, x√°c ƒë·ªãnh √¢m**.*

- N·∫øu $A$  l√† m·ªôt m√† tr·∫≠n (n·ª≠a) x√°c ƒë·ªãnh d∆∞∆°ng th√¨ $f(\mathbf{x})$ l√† m·ªôt h√†m l·ªìi.
- N·∫øu $A$  l√† m·ªôt m√† tr·∫≠n (n·ª≠a) x√°c ƒë·ªãnh √¢m th√¨ $f(\mathbf{x})$ l√† m·ªôt h√†m l√µm.

*‚Äú·ª¶a v·∫≠y sao m√† bi·∫øt ma tr·∫≠n m√¨nh ƒëang x√©t l√† ·ªü d·∫°ng n√†o?‚Äù.*  ƒê∆°n gi·∫£n gi·∫£n l√† ta ch·ª©ng minh c√°c b·∫•t ƒë·∫≥ng th·ª©c sau v·ªõi ma tr·∫≠n ƒëang x√©t (trong tr∆∞·ªùng h·ª£p n√†y l√† $A$) , $\forall \ z \in \mathbb{R}^n$

- Ma tr·∫≠n n·ª≠a x√°c ƒë·ªãnh d∆∞∆°ng n·∫øu : $z^TAz \geq 0$
- Ma tr·∫≠n x√°c ƒë·ªãnh d∆∞∆°ng n·∫øu :        $z^TAz > 0$
- Ma tr·∫≠n n·ª≠a x√°c ƒë·ªãnh √¢m n·∫øu :       $z^TAz \leq 0$
- Ma tr·∫≠n x√°c ƒë·ªãnh √¢m n·∫øu :              $z^TAz < 0$

M√† th∆∞·ªùng th√¨ ch√∫ng ta √≠t x√©t tr∆∞·ªùng h·ª£p n√†y l·∫Øm. c√≥ m·ªôt m·∫πo l√† : ‚ÄúV·ªõi m·ªôt ma tr·∫≠n c·∫•p b·∫•t k√¨ th√¨ n·∫øu n√≥ nh√¢n v·ªõi ho√°n v·ªã c·ªßa ch√≠nh n√≥ th√¨ s·∫Ω c√≥ ƒë∆∞·ª£c m·ªôt ma tr·∫≠n m·ªõi l√† ma tr·∫≠n ƒë·ªëi x·ª©ng n·ª≠a x√°c ƒë·ªãnh d∆∞∆°ng‚Äù Bi·ªÉu di·ªÖn to√°n h·ªçc : V·ªõi $U \in \mathbb{R}^{n \times m}$ th√¨  $UU^T \geq 0$   v√† $U^TU \geq 0$.

Well‚Ä¶ Ch·∫Øc l√† ƒë·ªß m·∫Øm mu·ªëi r·ªìi, c√πng nh·∫£y v√†o ki·∫øm nghi·ªám t·ªëi ∆∞u nh√© ü´£

### 2.4. Nghi·ªám c·ªßa b√†i to√°n Linear Regression

Nh∆∞ ƒë√£ ƒë·ªÅ c·∫≠p ·ªü tr√™n th√¨ m√¨nh c√≥ n√≥i l√† MSE l√† m·ªôt h√†m l·ªìi (m√¨nh s·∫Ω n√≥i t·∫°i sao n√≥ l·ªìi sau) v·∫≠y n√™n n√≥ s·∫Ω c√≥ m·ªôt gi√° tr·ªã t·ªëi ∆∞u cho h√†m MSE l√† nh·ªè nh·∫•t. V·∫≠y nh∆∞ ·ªü c·∫•p ba, c√°ch ƒë·ªÉ t√¨m gi√° tr·ªã t·ªëi ∆∞u n√†y ƒë√≥ l√† ƒë·∫°o h√†m l√™n v√† gi·∫£i ph∆∞∆°ng tr√¨nh ƒë·∫°o h√†m b·∫±ng 0. Nh∆∞ng m√† l√†m c√°ch n√†o ƒë·ªÉ ƒë·∫°o h√†m ƒë∆∞·ª£c h√†m ƒë√≥, n·∫øu ƒë√£ ƒë·ªçc ph·∫ßn 3 th√¨ m√¨nh s·∫Ω ch·ª©ng minh cho c√°c b·∫°n th·∫•y MSE l√† d·∫°ng to√†n ph∆∞∆°ng v√† l√† h√†m l·ªìi nh√©.

![alt text](../assets/ml_2/prove1.png)
<!-- $\begin{matrix}
\mathcal{L}(\mathbf{w})= \frac{1}{2} \| \color{Lime}{\mathbf{y}} \color{None}{-} \color{teal}{ \mathbf{\bar{X}}} \color{None}{\mathbf{w} \|_2^2} \\~~~~~~~~~~~~~~~~~~~~~~~~~~= \frac{1}{2}(\color{Lime}{\mathbf{y}} \color{None}{-} \color{teal}{ \mathbf{\bar{X}}} \color{None}{\mathbf{w})^T \ (}\color{Lime}{\mathbf{y}} \color{None}{-} \color{teal}{ \mathbf{\bar{X}}} \color{None}{\mathbf{w}})\\~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~=\frac{1}{2}(\color{Lime}{\mathbf{y}}^T \color{None}- \color{None}\mathbf{w}^T\color{teal}{\mathbf{\bar{X}}}^T \color{None})\ (\color{Lime}{\mathbf{y}} \color{None}- \color{teal}{\mathbf{\bar{X}}} \color{None}\mathbf{w}) \\~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~=\frac{1}{2}(\color{Lime}{\mathbf{y}}^T\color{Lime}{\mathbf{y}}\color{None} - \color{Lime}{\mathbf{y}}^T \color{teal}{\mathbf{\bar{X}}} \color{None}\mathbf{w} - \mathbf{w}^T \color{teal}{\mathbf{\bar{X}}}\color{Lime}{\mathbf{y}} \color{None}+ \mathbf{w}^T \color{teal}{\mathbf{\bar{X}}}^T\color{teal}{\mathbf{\bar{X}}}\color{None}\mathbf{w})\\~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~=\frac{1}{2}\color{None} \mathbf{w}^T \color{teal}{\mathbf{\bar{X}}}^T\color{teal}{\mathbf{\bar{X}}}\color{None}\mathbf{w} - \color{Lime}{\mathbf{y}}^T \color{teal}{\mathbf{\bar{X}}} \color{None}\mathbf{w} \ +\frac{1}{2} \color{Lime}{\mathbf{y}}^T\color{Lime}{\mathbf{y}}
\end{matrix}$ -->

V·∫≠y ta ƒë√£ nh√¨n ra ƒë∆∞·ª£c d·∫°ng to√†n ph∆∞∆°ng c·ªßa MSE v·ªõi ma tr·∫≠n $A$  t∆∞∆°ng ·ª©ng l√† $\color{teal}{\mathbf{\bar{X}}}^T\color{teal}{\mathbf{\bar{X}}}$ 

M√† $\color{teal}{\mathbf{\bar{X}}}^T\color{teal}{\mathbf{\bar{X}}}$ l√† m·ªôt ma tr·∫≠n ƒë·ªëi x·ª©ng n·ª≠a x√°c ƒë·ªãnh d∆∞∆°ng n√™n h√†m MSE l√† m·ªôt h√†m l·ªìi (ƒëpcm :v). 

<div style="text-align: center;">
    <img src="../assets/ml_2/mse_dim1.png" alt="mse_1" width="347" style="display: inline-block;">
    <img src="../assets/ml_2/mse_dim2.png" alt="mse_2" width="200" style="display: inline-block;">
</div>
<figcaption style="text-align: center;"><i>H√¨nh 4. V√≠ d·ª• v·ªÅ h√†m MSE trong kh√¥ng gian 1 v√† 2 chi·ªÅu</i></figcaption>

V√† √°p d·ª•ng c√¥ng th·ª©c ƒë·∫°o h√†m b·∫≠c m·ªôt $(\frac{\partial{\mathcal{L}(\mathbf{w})}}{\partial{\mathbf{w}}} = \color{teal}{\mathbf{\bar{X}}}^T\color{teal}{\mathbf{\bar{X}}}\color{None}\mathbf{w} - \color{teal}{\mathbf{\bar{X}}}^T\color{Lime}{\mathbf{y}} \color{None})$ v√† gi·∫£i ph∆∞∆°ng tr√¨nh ƒë·∫°o h√†m ƒë√≥ b·∫±ng kh√¥ng ta c√≥  .

![alt text](../assets/ml_2/prove2.png)
<!-- $\begin{matrix}
\frac{\partial{\mathcal{L}(\mathbf{w})}}{\partial{\mathbf{w}}} = 0~~~~~~~~~~~~~~~~~~~~ \\\Leftrightarrow \color{teal}{\mathbf{\bar{X}}}^T\color{teal}{\mathbf{\bar{X}}}\color{None}\mathbf{w} - \color{teal}{\mathbf{\bar{X}}}^T\color{Lime}{\mathbf{y}} \color{None} = 0 \\\Leftrightarrow \color{teal}{\mathbf{\bar{X}}}^T\color{teal}{\mathbf{\bar{X}}}\color{None}\mathbf{w}  ~~~=~~~~  \color{teal}{\mathbf{\bar{X}}}^T\color{Lime}{\mathbf{y}} \color{None} \\\Leftrightarrow \color{None}\mathbf{w} =~  (\color{teal}{\mathbf{\bar{X}}}^T\color{teal}{\mathbf{\bar{X}}}\color{None})^{-1}\color{teal}{\mathbf{\bar{X}}}^{T}\color{Lime}{\mathbf{y}} \color{None}
\end{matrix}$ -->

Trong tr∆∞·ªùng h·ª£p $\color{teal}{\mathbf{\bar{X}}}^T\color{teal}{\mathbf{\bar{X}}}$ kh√¥ng kh·∫£ ngh·ªãch, ta s·∫Ω c√≥ m·ªôt kh√°i ni·ªám g·ªçi l√† [Gi·∫£ Ngh·ªãch ƒê·∫£o](https://vi.wikipedia.org/wiki/Gi%E1%BA%A3_ngh%E1%BB%8Bch_%C4%91%E1%BA%A3o_Moore%E2%80%93Penrose). V·∫≠y tr∆∞·ªùng h·ª£p n√†y, nghi·ªám t·ªëi ∆∞u c·ªßa b·∫°n to√°n linear regression s·∫Ω c√≥ d·∫°ng:

$$
\color{None}{\mathbf{w}^* =~  (}\color{teal}{\mathbf{\bar{X}}}^T\color{teal}{\mathbf{\bar{X}}}\color{None}{)^{\dagger}} \color{teal}{\mathbf{\bar{X}}}^{T}\color{Lime}{\mathbf{y}} 
$$

**L∆ØU √ù :** Trong t·∫•t c·∫£ c√°c thu·∫≠t to√°n Machine Learning th√¨ ch·ªâ duy nh·∫•t Linear Regression c√≥ c√¥ng th·ª©c gi·∫£i nghi·ªám t·ªëi ∆∞u (v√¨ gi·∫£i ph∆∞∆°ng t√¨nh ƒë·∫°o h√†m b·∫±ng kh√¥ng d∆∞·ªùng nhi b·∫•t kh·∫£ thi). M√† v·ªõi tr∆∞·ªùng h·ª£p b·ªô d·ªØ li·ªáu qu√° l·ªõn, m√°y t√≠nh s·∫Ω kh√¥ng c√≥ ƒë·ªß t√†i nguy√™n ƒë·ªÉ t√≠nh to√°n ngh·ªãch ƒë·∫£o c·ªßa ma tr·∫≠n tr√™n. 

V·∫≠y th√¨ h·ªç x√†i g√¨ ƒë·ªÉ t√¨m nghi·ªám t·ªëi ∆∞u? C√¢u tr·∫£ l·ªùi ƒë√≥ ch√≠nh l√† **Gradient Descent.** 

### 2.5. Gradient Descent cho Linear Regression

‚Äú*Ph·∫ßn n√†y th√¨ m√¨nh s·∫Ω ch·ªâ n√≥i kh√°i ni·ªám ƒë∆°n gi·∫£n cho c√°c b·∫°n hi·ªÉu th√¥i, n·ªôi dung ƒë·∫ßy ƒë·ªß v√† chi ti·∫øt c√≥ l·∫Ω s·∫Ω n·∫±m ·ªü c√°c ph·∫ßn kh√°c.‚Äù*

T·ª´ quan s√°t nh∆∞ sau: N·∫øu b·∫°n ƒë·ª©ng tr√™n m·ªôt thung l≈©ng v√† th·∫£ m·ªôt vi√™n bi v√†o ƒë√≥ th√¨  n√≥ s·∫Ω t·ª± lƒÉn t·ªõi ƒëi·ªÉm th·∫•p nh·∫•t trong thung l≈©ng ·∫•y. √ù t∆∞·ªüng c·ªßa Gradient Descent l√† v·∫≠y.

V·ªõi m·ªôt h√†m m·∫•t m√°t l√† h√†m l·ªìi, ta s·∫Ω b·∫Øt ƒë·∫ßu kh·ªüi t·∫°o b·ªô tham s·ªë ·ªü v·ªã tr√≠ ng·∫´u nhi√™n, b·ªô tham s·ªë n√†y s·∫Ω t·ª´ t·ª´ ti·∫øn ƒë·∫øn gi√° tr·ªã t·ªëi ∆∞u m√† ta c·∫ßn t√¨m.

<figure style="text-align: center;">
  <img src="../assets/ml_2/1dimg_5_0.1_5.gif" width="300" alt="Gradient Descent">
  <figcaption><i>H√¨nh 5. V√≠ d·ª• v·ªÅ gradient descent (Ngu·ªìn : <a href="https://machinelearningcoban.com/">machinelearingcoban</a>)</i></figcaption>
</figure>


V·ªõi quan s√°t ·∫•y, h·ªç d·ª±a v√†o kh√°i ni·ªám Gradient(c√≥ th·ªÉ coi ƒë√¢y l√† ƒë·∫°o h√†m c·∫•p 1) n√≥i ng·∫Øn g·ªçn: ƒê·ªëi v·ªõi h√†m l·ªìi th√¨ ·ªü ph√≠a b√™n tr√°i c·ªßa nghi·ªám t·ªëi ∆∞u s·∫Ω c√≥ ƒë·∫°o h√†m b·∫±ng √¢m, c√°c ƒëi·ªÉm b√™n ph·∫£i s·∫Ω c√≥ ƒë·∫°o h√†m d∆∞∆°ng. H√†m l√µm th√¨ ng∆∞·ª£c l·∫°i. 

<figure style="text-align: center;">
  <img src="../assets/ml_2/gradient.png" width="400" alt="Gradient Descent">
  <figcaption><i>H√¨nh 6. ƒê·ªô d·ªëc c·ªßa m·ªôt h√†m l√µm </i></figcaption>
</figure>
 

V·∫≠y c√¥ng th·ª©c c·ªßa Gradient Descent nh∆∞ sau : 

![bug latex dcm](../assets/ml_2/gradient_fomula.png)

Gi·∫£i th√≠ch s∆° qua th√¨ d·ª±a v√†o v·ªã tr√≠ b·ªô tham s·ªë hi·ªán t·∫°i th√¨ b·ªô tham s·ªë ti·∫øp theo s·∫Ω ti·∫øn g·∫ßn t·ªõi t·ªõi ƒëi·ªÉm t·ªëi ∆∞u $\mathbf{w}^*$ v·ªõi kho·∫£ng c√°ch l√† $\eta$  l·∫ßn gi√° tr·ªã ƒë·∫°o h√†m ·ªü th·ªùi ƒëi·ªÉm hi·ªán t·∫°i.

V·ªõi $\eta$  l√† s·ªë d∆∞∆°ng g·ªçi l√† *Learning rate* bi·ªÉu di·ªÖn t·ªëc ƒë·ªô h·ªçc. D·∫•u  tr·ª´ th·ªÉ hi·ªán vi·ªác ta s·∫Ω ƒëi ng∆∞·ª£c v·ªõi ƒë·∫°o h√†m (ƒê√¢y l√† l√Ω do n√≥ c√≥ t√™n Gradient Descent).

C√≥ m·ªôt l∆∞u √Ω nh·ªè l√† ta c·∫ßn ph·∫£i l·ª±a ch·ªçn $\eta$  m·ªôt c√°ch h·ª£p l√≠ th√¨ n·∫øu qu√° nh·ªè th√¨ n√≥ s·∫Ω h·ªôi t·ª• r·∫•t l√¢u (ƒëi ƒë·∫øn ƒëi·ªÉm t·ªëi ∆∞u r·∫•t ch·∫≠m), c√≤n n·∫øu qu√° l·ªõn th√¨ c√≥ th·ªÉ s·∫Ω ph√¢n k·ª≥ (kh√¥ng ch·∫°m ƒë∆∞·ª£c t·ªõi ƒëi·ªÉm t·ªëi ∆∞u) t·ª©c l√† s·∫Ω b·∫≠t qua b·∫≠t l·∫°i 2 b√™n ƒëi·ªÉm t·ªëi ∆∞u.

<div style="text-align: center;">
    <figure style="display: inline-block;">
        <img src="../assets/ml_2/small_eta.gif" width="300" alt="small eta">
        <figcaption><i>H√¨nh 7. H·ªôi t·ª• ch·∫≠m khi eta th·∫•p </i></figcaption>
    </figure>
    <figure style="display: inline-block;">
        <img src="../assets/ml_2/big_eta.gif" width="300" alt="big eta">
        <figcaption><i>H√¨nh 8. Ph√¢n k·ª≥ khi eta l·ªõn </i></figcaption>
    </figure>
</div>

V·∫≠y l√†m sao ƒë·ªÉ ch·ªçn $\eta$ h·ª£p l√Ω. C√¢u tr·∫£ l·ªùi th∆∞·ªùng s·∫Ω l√† kh√¥ng c√≥ c√°ch ch·ªçn sao cho h·ª£p l√Ω m√† b·∫°n s·∫Ω ph·∫£i t·ª± th·ª±c nghi·ªám ƒë·ªÉ t√¨m ra $\eta$ h·ª£p l√Ω nh·∫•t cho b√†i to√°n c·ªßa m√¨nh.

## III. Th·ª±c H√†nh

Quay l·∫°i v·ªõi v√≠ d·ª• **XanhSM** ·ªü ƒë·∫ßu b√†i nh√©. D·ªØ li·ªáu c·ªßa m√¨nh c√≥  s·ªë chi·ªÅu l√† 3 n√™n m√¨nh s·∫Ω s·∫Ω t√¨m m·ªôt m·∫∑t ph·∫≥ng (2 chi·ªÅu) c√≥ th·ªÉ ƒëi qua h·∫øt c√°c ƒëi·ªÉm d·ªØ li·ªáu n√†y. 

M√¨nh s·∫Ω ƒë∆∞a k·∫øt qu·∫£ t·ª´ nh∆∞ng ƒëo·∫°n code, s·∫Ω  s·ª≠ d·ª•ng plotly ƒë·ªÉ bi·ªÉu di·ªÖn ƒë·ªì th·ªã 3 chi·ªÅu

```python
import numpy as np
import plotly.graph_objects as go
```

### 3.1 Hi·ªÉn th·ªã d·ªØ li·ªáu:

ƒê·∫ßu ti√™n s·∫Ω nh·∫≠p d·ªØ li·ªáu v√† v·∫Ω theo t·ªça ƒë·ªô xyz.

```python
X = np.array([[12,2,6,7,9,15,1,20,18,50,31,14],
              [24,4,25,13,17,10, 3,40,4,90,62,28]]).T
y = np.array([60,13,20,36,40,126,10,90,300,202,128,72]).reshape(-1,1)

fig = go.Figure(data=[go.Scatter3d(
    x=X[:,0],
    y=X[:,1],
    z=y.ravel(),
    mode='markers',
    marker=dict(
        size=8,
        color=y.ravel(),               
        colorscale='Viridis',   
        opacity=0.8
    )
)])

fig.update_layout(
    scene = dict(
        xaxis_title='S·ªë Km',
        yaxis_title='Th·ªùi gian',
        zaxis_title='Gi√° ti·ªÅn'
    ),
    margin=dict(l=0, r=0, b=0, t=0)
)
fig.show()
```

<figure style="text-align: center;">
    <img src="../assets/ml_2/lab1.png" width="500" alt="visualize">
    <figcaption><i>H√¨nh 9. Tr·ª±c quan d·ªØ li·ªáu </i></figcaption>
</figure>

### 3.2 Gi·∫£i b·∫±ng ph∆∞∆°ng ph√°p Least Square

ƒê·∫ßu ti√™n s·∫Ω √°p d·ª•ng c√¥ng th·ª©c Least Square ƒë·ªÉ gi·∫£i nghi·ªám.

```python
from numpy import linalg as LA

one = np.ones((X.shape[0], 1))
Xbar = np.concatenate((one,X), axis = 1)
# ap d·ª•ng cong thuc t√¨m nghiem:
result_least_square = LA.pinv(Xbar.T @ Xbar) @ Xbar.T @ y
print("w0 = ", result_least_square[0])
print("w1 = ", result_least_square[1])
print("w2 = ", result_least_square[2])
```

Output :

`w0 =  [25.60877573] 
w1 =  [13.37537191] 
w2 =  [-5.26484098]`

<div style="text-align: center;">
    <img src="../assets/ml_2/lab2.png" alt="visualize" width="300" style="display: inline-block;">
    <img src="../assets/ml_2/lab3.png" alt="visualize" width="336" style="display: inline-block;">
</div>
<figcaption style="text-align: center;"><i>H√¨nh 10. Bi·ªÉu di·ªÖn m·∫∑t ph·∫≥ng trong t·ª´ nghi·ªám v·ª´a t√¨m ƒë∆∞·ª£c</i></figcaption>

### 3.3 Gi·∫£i b·∫±ng ph∆∞∆°ng ph√°p Gradient Descent

```python
class LinearRegression: 
    def __init__(self, learning_rate=0.05, #learning rate
							      iterations=600, # epoch
							      tolerance=1e-3, # eps
							       clip_value=1.0) # clip to ignore gradient Explode:
        self.learning_rate = learning_rate
        self.iterations = iterations
        self.tolerance = tolerance
        self.clip_value = clip_value
        self.weights, self.bias = self._initialize_parameters()

    def _initialize_parameters(self):
    '''
    Init Param from zeros
    '''
        weights = np.zeros((X.shape[1], 1))
        bias = 0
        return weights, bias

    def _compute_cost(self, y, y_hat):
		    """
		    MSE from scratch
		    """
        m = len(y)
        cost = (1 / (2 * m)) * np.sum((y_hat - y) ** 2)
        return cost

    def _compute_gradients(self, X, y, y_hat):
		    """
		    calculate gradient of MSE
		    """
        m = len(y)
        dw = (1 / m) * np.dot(X.T, (y_hat - y))
        db = (1 / m) * np.sum(y_hat - y)
        return dw, db

    def _clip_gradients(self, dw, db):
		    """
		    use to avoid gradient exploxe
			  """
        dw = np.clip(dw, -self.clip_value, self.clip_value)
        db = np.clip(db, -self.clip_value, self.clip_value)
        return dw, db

    def _update_parameters(self, dw, db):
		    """
			    gradient update
		    """
        self.weights = self.weights - self.learning_rate * dw
        self.bias = self.bias - self.learning_rate * db

    def train(self, X, y):
        for i in range(self.iterations):
            y_hat = np.dot(X, self.weights) + self.bias
            cost = self._compute_cost(y, y_hat)
            dw, db = self._compute_gradients(X, y, y_hat)
            dw, db = self._clip_gradients(dw, db)
            if np.linalg.norm(dw)/len(dw) < self.tolerance:
                print(f"Convergence reached at iteration {i}")
                break
            self._update_parameters(dw, db)
            if i % 50==0:
                print(f"Iteration {i}, cost: {cost}")

    def predict(self, X):
        return np.dot(X, self.weights) + self.bias

model = LinearRegression()
model.train(X, y)

print("Nghi·ªám t√¨m b·ªüi Gradient Descent")
print("w0 = ", model.bias)
print("w1 = ", model.weights[0][0])
print("w2 = ", model.weights[1][0])
```

`w0 =  23.548239494777462 
w1 =  13.220760543795164 
w2 =  -5.147517304599516`

### 3.4  So s√°nh k·∫øt qu·∫£ v·ªõi Sklearn

```python
from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_squared_error

model_sk = LinearRegression()
model_sk.fit(X, y)

predictions = model_sk.predict(X)
#
mse = mean_squared_error(y, predictions)
print(f"Mean Squared Error: {mse}")

print(f"w0 = {model_sk.intercept_[0]}")
print(f"w1 = {model_sk.coef_[0][0]}")
print(f"w2 = {model_sk.coef_[0][1]}")
```

`
 w0 = 25.608775733107606 
 w1 = 13.375371913472891 
 w2 = -5.26484097746805`

Nh·∫≠n th·∫•y r·∫±ng nghi·ªám c·ªßa Sklearn v√† ph∆∞∆°ng ph√°p LeastSquare l√† tr√πng kh·ªõp v·ªõi nhau, Gradient descent s·∫Ω cho k·∫øt qu·∫£ t·ªët nh∆∞ng s·∫Ω t·ªët h∆°n n·∫øu c√≥ m·ªôt k·ªπ thu·∫≠t training hi·ªáu qu·∫£ h∆°n (khum ch·ªâ ƒë√¢u).

## IV. K·∫øt lu·∫≠n

C·∫£m ∆°n c√°c b·∫°n ƒë√£ ƒë·ªçc t·ªõi nh·ªØng d√≤ng cu·ªëi c·ªßa b√†i vi·∫øt n√†y, ·ªü b√†i ti·∫øp theo ch√∫ng ta s·∫Ω qua thu·∫≠t to√°n ph√¢n lo·∫°i h·ªç h√†ng v·ªõi Linear Regression nh√©.

Ngo√†i ra c√≤n c√≥ nh·ªØng kh√°i ni·ªám m√† b·∫°n s·∫Ω g·∫∑p trong qu√° t√¨nh training m·ªôt thu·∫≠t to√°n ML nh∆∞ l√† [Vanishing gradient v√† Exploding gradient](https://en.wikipedia.org/wiki/Vanishing_gradient_problem). Nh∆∞ng v√¨ n·ªôi dung qu√° d√†i n√™n m√¨nh xin ph√©p ƒë·ªÉ d√†nh trong m·ªôt b√†i kh√°c nh√©.

**T√ÄI LI·ªÜU THAM KH·∫¢O :**
[Machine Learning c∆° b·∫£n (machinelearningcoban.com)](https://machinelearningcoban.com/)